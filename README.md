# artificial intelligence research papers.

The repository serves as both a personal research log and a knowledge-sharing hub, covering topics across machine learning, deep learning, representation learning, large language models, and AI systems.

Each entry or notebook captures the core ideas, mathematical intuitions, experimental takeaways, and practical applications from the papers studied â€” transforming complex research into a clear, actionable understanding.

The goal is to bridge theory and practice by documenting how research concepts can be reproduced, extended, or applied in real-world machine learning systems.



## Research Papers:

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)

- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)

- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)

- [The Rise and Potential of Large Language Model-Based Agents: A Survey](https://arxiv.org/pdf/2309.07864)

- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961)

- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108)

- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339)

- [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol)

- [A Comprehensive Overview of Large Language Models](https://arxiv.org/pdf/2307.06435)

- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223)

- [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363)

## Stanford CME295

### Lecture 1 - Transformer

- [cheatsheet-transformers-large-language-models.pdf](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/en/cheatsheet-transformers-large-language-models.pdf)

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)

- [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473)

- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)


### Lecture 2 - Transformer-Based Models & Tricks

- []()

- []()

- []()
